<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta http-equiv="x-ua-compatible" content="IE=9" >

<title>Prediction of Activity Classes from the Weight Lifting Exercise Dataset (HAR)</title>

<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 12px;
   margin: 8px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 { 
   font-size:2.2em; 
}

h2 { 
   font-size:1.8em; 
}

h3 { 
   font-size:1.4em; 
}

h4 { 
   font-size:1.0em; 
}

h5 { 
   font-size:0.9em; 
}

h6 { 
   font-size:0.8em; 
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre {	
   margin-top: 0;
   max-width: 95%;
   border: 1px solid #ccc;
   white-space: pre-wrap;
}

pre code {
   display: block; padding: 0.5em;
}

code.r, code.cpp {
   background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * { 
      background: transparent !important; 
      color: black !important; 
      filter:none !important; 
      -ms-filter: none !important; 
   }

   body { 
      font-size:12pt; 
      max-width:100%; 
   }
       
   a, a:visited { 
      text-decoration: underline; 
   }

   hr { 
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote { 
      padding-right: 1em; 
      page-break-inside: avoid; 
   }

   tr, img { 
      page-break-inside: avoid; 
   }

   img { 
      max-width: 100% !important; 
   }

   @page :left { 
      margin: 15mm 20mm 15mm 10mm; 
   }
     
   @page :right { 
      margin: 15mm 10mm 15mm 20mm; 
   }

   p, h2, h3 { 
      orphans: 3; widows: 3; 
   }

   h2, h3 { 
      page-break-after: avoid; 
   }
}

</style>

<!-- Styles for R syntax highlighter -->
<style type="text/css">
   pre .operator,
   pre .paren {
     color: rgb(104, 118, 135)
   }

   pre .literal {
     color: rgb(88, 72, 246)
   }

   pre .number {
     color: rgb(0, 0, 205);
   }

   pre .comment {
     color: rgb(76, 136, 107);
   }

   pre .keyword {
     color: rgb(0, 0, 255);
   }

   pre .identifier {
     color: rgb(0, 0, 0);
   }

   pre .string {
     color: rgb(3, 106, 7);
   }
</style>

<!-- R syntax highlighter -->
<script type="text/javascript">
var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.cpp=function(){var a={keyword:{"false":1,"int":1,"float":1,"while":1,"private":1,"char":1,"catch":1,"export":1,virtual:1,operator:2,sizeof:2,dynamic_cast:2,typedef:2,const_cast:2,"const":1,struct:1,"for":1,static_cast:2,union:1,namespace:1,unsigned:1,"long":1,"throw":1,"volatile":2,"static":1,"protected":1,bool:1,template:1,mutable:1,"if":1,"public":1,friend:2,"do":1,"return":1,"goto":1,auto:1,"void":2,"enum":1,"else":1,"break":1,"new":1,extern:1,using:1,"true":1,"class":1,asm:1,"case":1,typeid:1,"short":1,reinterpret_cast:2,"default":1,"double":1,register:1,explicit:1,signed:1,typename:1,"try":1,"this":1,"switch":1,"continue":1,wchar_t:1,inline:1,"delete":1,alignof:1,char16_t:1,char32_t:1,constexpr:1,decltype:1,noexcept:1,nullptr:1,static_assert:1,thread_local:1,restrict:1,_Bool:1,complex:1},built_in:{std:1,string:1,cin:1,cout:1,cerr:1,clog:1,stringstream:1,istringstream:1,ostringstream:1,auto_ptr:1,deque:1,list:1,queue:1,stack:1,vector:1,map:1,set:1,bitset:1,multiset:1,multimap:1,unordered_set:1,unordered_map:1,unordered_multiset:1,unordered_multimap:1,array:1,shared_ptr:1}};return{dM:{k:a,i:"</",c:[hljs.CLCM,hljs.CBLCLM,hljs.QSM,{cN:"string",b:"'\\\\?.",e:"'",i:"."},{cN:"number",b:"\\b(\\d+(\\.\\d*)?|\\.\\d+)(u|U|l|L|ul|UL|f|F)"},hljs.CNM,{cN:"preprocessor",b:"#",e:"$"},{cN:"stl_container",b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:a,r:10,c:["self"]}]}}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
hljs.initHighlightingOnLoad();
</script>




</head>

<body>
<h1>Prediction of Activity Classes from the Weight Lifting Exercise Dataset (HAR)</h1>

<p>The Weight Lifting Exercise Dataset is used for Human Activity Recognition (HAR) 
and contains data from accelerometeres attached to the test subjects&#39; forearm, 
arm, and belt. The subjects were asked to perform dumbbell curls in five different
manners (classes): exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). This information is contained in the <code>classe</code> variable of the dataset.</p>

<p>Goal of this analysis is to predict the class from the available data. 
For that purpose, we will train three models (random forest, multinomial
regression, and k nearest neighbor) and build an ensemble model by stacking. 
The data </p>

<p>was downloaded from the <a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv">Coursera page</a>. </p>

<h2>Prerequisites</h2>

<p>In order to perform the analysis, some <code>R</code> libraries need to be loaded. Most of the
analyses are carried out with the <code>caret</code> package:</p>

<pre><code class="r">library(caret)
library(randomForest)
library(nnet)
library(e1071)
</code></pre>

<h2>Preprocessing the data</h2>

<p>First, the downloaded data is read into <code>R</code> via the <code>read.csv()</code> function:</p>

<pre><code class="r">path.raw &lt;- &quot;C:/data-sync/coursera/data-science-08-machine-learning/assignment01/&quot;
path.dat &lt;- paste0(path.raw, &quot;&quot;)
setwd(path.dat)
dat.all.raw &lt;- read.csv(&quot;pml-training.csv&quot;)
</code></pre>

<p>The data seems to contain individual measurements as well as summary measurements,
as indicated by the fact that some variables are missing when the variable <code>new_window</code>
has the value <code>&quot;no&quot;</code> (not shown here for brevity). These missing variables relate to 
skew and kurtosis of measurements, which can only be calulated as a summary 
on multiple measurements. These summary measurements (<code>new_window==&quot;yes&quot;</code>) are removed from the data.</p>

<pre><code class="r">dat.all &lt;- subset(dat.all.raw, dat.all.raw$new_window != &quot;yes&quot;)
</code></pre>

<pre><code class="r">## vars:
vars.all &lt;- c(&quot;classe&quot;, &quot;roll_belt&quot;, &quot;pitch_belt&quot;, &quot;yaw_belt&quot;, &quot;total_accel_belt&quot;, 
    &quot;gyros_belt_x&quot;, &quot;gyros_belt_y&quot;, &quot;gyros_belt_z&quot;, &quot;accel_belt_x&quot;, &quot;accel_belt_y&quot;, 
    &quot;accel_belt_z&quot;, &quot;magnet_belt_x&quot;, &quot;magnet_belt_y&quot;, &quot;magnet_belt_z&quot;, &quot;roll_arm&quot;, 
    &quot;pitch_arm&quot;, &quot;yaw_arm&quot;, &quot;total_accel_arm&quot;, &quot;gyros_arm_x&quot;, &quot;gyros_arm_y&quot;, 
    &quot;gyros_arm_z&quot;, &quot;accel_arm_x&quot;, &quot;accel_arm_y&quot;, &quot;accel_arm_z&quot;, &quot;magnet_arm_x&quot;, 
    &quot;magnet_arm_y&quot;, &quot;magnet_arm_z&quot;, &quot;roll_dumbbell&quot;, &quot;pitch_dumbbell&quot;, &quot;yaw_dumbbell&quot;, 
    &quot;gyros_dumbbell_x&quot;, &quot;gyros_dumbbell_y&quot;, &quot;gyros_dumbbell_z&quot;, &quot;accel_dumbbell_x&quot;, 
    &quot;accel_dumbbell_y&quot;, &quot;accel_dumbbell_z&quot;, &quot;magnet_dumbbell_x&quot;, &quot;magnet_dumbbell_y&quot;, 
    &quot;magnet_dumbbell_z&quot;, &quot;roll_forearm&quot;, &quot;pitch_forearm&quot;, &quot;yaw_forearm&quot;, &quot;total_accel_forearm&quot;, 
    &quot;gyros_forearm_x&quot;, &quot;gyros_forearm_y&quot;, &quot;gyros_forearm_z&quot;, &quot;accel_forearm_x&quot;, 
    &quot;accel_forearm_y&quot;, &quot;accel_forearm_z&quot;, &quot;magnet_forearm_x&quot;, &quot;magnet_forearm_y&quot;, 
    &quot;magnet_forearm_z&quot;)

dat.all.clean &lt;- dat.all[vars.all]
</code></pre>

<h2>Splitting the data in training, validation, and test set</h2>

<p>In order to train the machine learning models, the data is split into three parts. 
The training set includes a third of the data, a validation set consists of 
another third, and finally a test set consisting of the final third of the data. 
The validation set will 
be used to evaluate the out-of-sample error of the machine learning models and 
to train a stacked model.</p>

<p>The fraction of a third of the data for the training set was chosen
due to the fact that computing the models (especially the random forest models,
see below) takes quite some time on my machine (about 30 min).</p>

<pre><code class="r">set.seed(42)
wch.fold &lt;- createFolds(dat.all.clean$classe, k = 3, list = FALSE)
dat.train &lt;- dat.all.clean[wch.fold == 1, ]
dat.val &lt;- dat.all.clean[wch.fold == 2, ]
dat.test &lt;- dat.all.clean[wch.fold == 3, ]
</code></pre>

<h2>Training individual models</h2>

<p>This section describes the different models that were used. All of the models
were trained on the training data set, and then used to predict the classes
of the validation data set to get a first hint at the out-of-sample error. </p>

<h3>Random forest model</h3>

<p>Random forest models are widely used in machine learning because they often 
give very good out-of-the box predictions without knowing much about the data,
as they is not influenced by outliers, and also take into account 
interactions by design.
Hence, a random forest model is used as a starting point here. </p>

<p>It is fitted with the standard options of the <code>caret</code> package, which uses
25 iterations of bootstrapping within the training data set to select 
the best model. </p>

<pre><code class="r">set.seed(12)
caret.rf &lt;- train(classe ~ roll_belt + pitch_belt + yaw_belt + total_accel_belt + 
    gyros_belt_x + gyros_belt_y + gyros_belt_z + accel_belt_x + accel_belt_y + 
    accel_belt_z + magnet_belt_x + magnet_belt_y + magnet_belt_z + roll_arm + 
    pitch_arm + yaw_arm + total_accel_arm + gyros_arm_x + gyros_arm_y + gyros_arm_z + 
    accel_arm_x + accel_arm_y + accel_arm_z + magnet_arm_x + magnet_arm_y + 
    magnet_arm_z + roll_dumbbell + pitch_dumbbell + yaw_dumbbell + gyros_dumbbell_x + 
    gyros_dumbbell_y + gyros_dumbbell_z + accel_dumbbell_x + accel_dumbbell_y + 
    accel_dumbbell_z + magnet_dumbbell_x + magnet_dumbbell_y + magnet_dumbbell_z + 
    roll_forearm + pitch_forearm + yaw_forearm + total_accel_forearm + gyros_forearm_x + 
    gyros_forearm_y + gyros_forearm_z + accel_forearm_x + accel_forearm_y + 
    accel_forearm_z + magnet_forearm_x + magnet_forearm_y + magnet_forearm_z, 
    data = dat.train, method = &quot;rf&quot;)
</code></pre>

<p>The training time of this algorithm is actually quite long, on my machine this
took 33 minutes. After training the model, we can predict the classes in the
training data set and in the validation data set:</p>

<pre><code class="r">pred.caret.rf.train &lt;- predict(caret.rf$finalModel)
pred.caret.rf.val &lt;- predict(caret.rf$finalModel, newdata = dat.val)
</code></pre>

<p>The performance in the training set is really good:</p>

<pre><code class="r">confusionMatrix(pred.caret.rf.train, dat.train$classe)
</code></pre>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1817   24    0    0    0
##          B    3 1201   21    1    4
##          C    2   12 1090   21    3
##          D    0    0    6 1026    6
##          E    1    2    0    1 1163
## 
## Overall Statistics
##                                        
##                Accuracy : 0.983        
##                  95% CI : (0.98, 0.986)
##     No Information Rate : 0.285        
##     P-Value [Acc &gt; NIR] : &lt;2e-16       
##                                        
##                   Kappa : 0.979        
##  Mcnemar&#39;s Test P-Value : NA           
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity             0.997    0.969    0.976    0.978    0.989
## Specificity             0.995    0.994    0.993    0.998    0.999
## Pos Pred Value          0.987    0.976    0.966    0.988    0.997
## Neg Pred Value          0.999    0.993    0.995    0.996    0.998
## Prevalence              0.285    0.193    0.174    0.164    0.184
## Detection Rate          0.284    0.188    0.170    0.160    0.182
## Detection Prevalence    0.287    0.192    0.176    0.162    0.182
## Balanced Accuracy       0.996    0.982    0.984    0.988    0.994
</code></pre>

<p>Actually, the performance is so good that it seems likely we have overfitted
our model to the data. To test this assumption, we check on the performance
of the model in the validation data set.</p>

<pre><code class="r">confusionMatrix(pred.caret.rf.val, dat.val$classe)
</code></pre>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1809   16    0    0    0
##          B    3 1206   16    1    2
##          C   10   18 1100   23    5
##          D    1    0    1 1024    5
##          E    1    0    0    1 1164
## 
## Overall Statistics
##                                         
##                Accuracy : 0.984         
##                  95% CI : (0.981, 0.987)
##     No Information Rate : 0.285         
##     P-Value [Acc &gt; NIR] : &lt; 2e-16       
##                                         
##                   Kappa : 0.98          
##  Mcnemar&#39;s Test P-Value : 1.22e-07      
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity             0.992    0.973    0.985    0.976    0.990
## Specificity             0.997    0.996    0.989    0.999    1.000
## Pos Pred Value          0.991    0.982    0.952    0.993    0.998
## Neg Pred Value          0.997    0.993    0.997    0.995    0.998
## Prevalence              0.285    0.194    0.174    0.164    0.184
## Detection Rate          0.282    0.188    0.172    0.160    0.182
## Detection Prevalence    0.285    0.192    0.180    0.161    0.182
## Balanced Accuracy       0.994    0.984    0.987    0.987    0.995
</code></pre>

<p>Given that the performance of the prediction algorithm remains almost unchanged
in the validation data set, we seem not to have overfitted the data. </p>

<p>Regarding the variable importance gives some clue to which variables 
are most important:</p>

<pre><code class="r">plot(varImp(caret.rf))
</code></pre>

<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAfgAAAH4CAMAAACR9g9NAAAAw1BMVEUAAAAAACAAACgAADoAAGYAOjoAOmYAOpAAZrYAgP8hWLYoABcoOjI6AAA6ADo6AGY6Ojo6OmY6OpA6ZmY6ZpA6ZrY6kNtmAABmADpmAGZmIABmOgBmOpBmZjpmZmZmZpBmkNtmtv972/+QOgCQOjqQOmaQZgCQZmaQkDqQkGaQkLaQtpCQvJCQ27aQ2/+2ZgC2Zjq2kJC225C2/7a2///bkDrbkGbbtmbb25Db29vb/9vb////tmb/25D//7b//9v///+Po8d+AAAACXBIWXMAAAsSAAALEgHS3X78AAAdnklEQVR4nO2dAZfctnWFbyKnkpsq3trRKmkix1srTVs7WsvVpk60a83//1UhCZIzw3kEcB9BYmb3fsdnvLozXMB+4gAX7xHATjxJULsDog6o3QFRB9TugKgDandA1AG1OyDqgNodEHVA7Q6IOqB2B0QdULsDog6o3QFRB9TugKgDandA1AG1OyDqgNodEHVA7Q6IOqB2B0QdULsDog6o3QFRB9TugKgDandA1AG1OyDqgNodEHVA7Q6IOqB2B0QdULsDog6o3QFRB9TugKgDandA1AG1OzABunybyxc2Vhzo8m0uX9hYcaDLt7l8YWPFgS7f5vKFjRUHunybyxc2Vhzo8m0uX9jYAiA2Ye5//4ahzmnZVoUbkPoyfv79h+af8PPDqyN99/5jrOV1+vMU+Pzzzy0ZMx+f05cRCfzPf1Tg1+Dzz+3IY+bzc7qLu6vPPtwCL44D/9XrRtk1+nWr37Z/mG+59oh4sdQN/IvdfRvjm6PAf/GhUVr97kZ3/GrUDfx1E9zd7v76KPDftMpd+7fyWoFfj5pjfBN4447/8mN7x1+3PyvwmwNSd9EEfnc6xl+97Md4dH8hXj//GGm5aH/E1rP6HOyWbVW4AakX4OFlO64/e0e1bKvCDUg9ycFazKFT393ORjqrZXd/zo2ZudbmgNRzGOZp07W5JT1a0p9zYs5dbQ5IPUJYkenXYhqb9vynRuhnbM0crl/D6Rdzhule88cfm49d346ftFuuuQZSkkcZ+G5FJjiz1qvdffXFh09v+y/4Rg6Orl/Mubtpvg+ad5s/Ntf9/PqmeSvWsqM/Z8mjDPyrYY3mjx87e94KP+wDH9Zw2sWc9q9+969n75o/tn8D/vzhiQT+MY7x0zv+/lfHge/v+OtdWLRpfmjv+KcW+HMBpB5hHOPbtZjmdv7sx+PA94P6db9o07i6Z29uFPhKgNQjHLk3P3bLtircgNQj2IGfXa55QoUY5zKuHwJSL8fTKcQ4m5n8ISB1nmbsbmLcju/NSP7z796NyztPphDjiQa+ifpo2m/ur/pE7FMqxHiigf/05/9rRvDOtD+8+tt3r4Z5/hMqxDjDuG8xxt99fd2b9k9vf/uPv/ypD7AKMaoCUnfQjOu7YNrb4rvb/ptehRh1Aak7aG9tArtlWxVuQOox3h8E+IfRuN8FDx/cW/s6OPvDOqxky57+bMc5DuIJQOoRxlG65Yfpis0+8OPnH03gz3LangDZ+sOYNu/W3D99i8/evGsH6qBc70Yn3ibfm7e6lfrxqmDbJ+Z9DHw7ub+O9aiiDU/zyAM/pM17T37dztqa8bv5uTfjwx3f/LF9KwR+vKqz7VPzvr/jx5mA3aO5fp4FjzzwYxKt8+RtMu2HPrq9GR8Cf3fw1nhVsO1T8z4Gfj8DtHs018/z4PLi7gn8mEgfb+vejB/c8c0HmlCGD/SB72z71LwfPl0R7dFcP4UTZOv7EHaevB/jQ8o9mPHX+wK7Z1fvxg/0V3W2fWre+8A3n0VirX7Jf6Q4BaQ+cndNVM4y2C3bqnADUh9pb9LriWbn3mcq7Xt/113yyy8uKR9/gSP6KSB1H1al/cXm4y9xDn8KSJ0iWml/sfl4BT5JtNL+YvPxCnySaKX95ebjH0Pct73jjyrtlY+vCkidIlppr3x8VUDqFK5Ke7tlWxVuQOoxTvLxhm2/7Hz8oxjdAyD1CI8/H/845vMBZOur5uND8X2kRzWN+8gTDfya+fiu+D7Wo7l+bsoTDfya+fhQfB/p0Vw/t+XxxP1c8vFd8X2sR3P9FE6Qra+Zjw/F97EezfVTOAGpjxTOx1926dUFAlIfycjHH93SM/vc9Ze82b9rt5zsT2ke0XBuAlJnOAz8pe1z95gm8CYg9QzMYwkubZ87BZ7HPJbg0va5U+B5zGMJLm6fu0ce95UCH7vjtc/dWQBSz8A8lkD73J0ZIPX1sVu2VeEGpB7hyLGNadnB259ZWvaxD+FJQOoR7MAPnFfgH/2kPQlIPdCN1l2l/MfuZcivj2+fez5egQepdwQLHirl25erIb8e3r6AfLwCD1IPdBa8c2Xdy5hf79+9gHz8U4+7L/DBgodK+fYFQ349vK18/AUAUu8Ydq0DPvvQvQz59f595ePPH5D6WigfvzEg9Sjjw+75zq6/5NfP/rraGP/kh3MTkHoGU39OWnq7ZX9/NIE3AanPY6bhHZbeblmWvTAg9XnMNLzD0tst8/0ZUOBNQOrzmGl4h6W3W+b7M6K4W4DU5zHT8A5Lb7fM90dEAanPY6bhHZbebpnvj4gCUl8fu2VbFW5A6rnMHjeXxG7ZVoUbkHoOyacootgt22oSTezmAKnn4HiKItmyraaQlZsFpD4hXpGR/xTF7Yv2n0jLWrwpDEj9mERFRv5TFLvbr4ftNOyW8/ozRYGfBaQ+IV6Rkf8Uxe7hN6ukZRX3OUDqxyQqMvKfovj09rsLPprkEgGpH5OoyMh+iuLTt813gTY43BKQ+vrYLduqcANSj7Hf4NBYvplJwv948km7ZU9/NMRHAKlHOL8NDjWpnwfZ+robHK7i4xX4eZCtr3vg4Co+XoGfB9n6ygcOruLjFfdZkK2vusGhfPzWIFtf98BB+fiNAamP6MDBywakPnK6weGRKc+1c2v5eI3uCUDqEc5pYwTN51OA1APFN0a4w/j9YbcsB18YkHpH+Y0RDp6atFuO9ucUBT4FSD1QfGOE4k/LKu4JQOod5TdG2Mdds/ptAKl3FN8YobUIiUOF2f8wEQekvj52y7Yq3IDUo8w/RTGk6id2bvf+J/n4OoDUfYzL+FMff5jCj7ZM9kez+hQgdZP8VP3Ex4+6fPzWgNRN8lP1Ux+/v+Pl47cFpG6Sn6qf+vgx8PLxGwNSN8lP1U99/KEebZnqj0gDUjfJT9VPfXyvy8dvDkg9h2WpertlWxVuQOo5RFL132/k4zXEpwCp+zjcnvxQXMvHa1KfBKSeJJ6q38jHK/BJQOopEqn6jXy8Ap8EpJ4knqrfyscr7ilA6ikSqXr5+HMBpJ4ikaqXjz8XQOrrY7dsq8INSN3J4Ne38PEa33MAqfvYMh+vGX0WIHWT5fn4ggcOKvBZgNRNlufjCx44qMBnAVI3WZ6PL3ngoOKeA0jdZHk+XgcObg1I3WRxPl4HDm4OSD0HTz5eBw5uDEg9h0//MaTa9vvVz5Xc9/qb/Tt2y/n90RCfBUg9h3HkrrFfvSb1eYDUd4xrz9+vvlw+XoHPA6S+Y1w7sV99sXy8Ap8HSH3HuHZmv/pikzvFPQuQ+o5x7fn71SsfvzUg9R3j2on96pWP3xiQ+imlN7yzW7ZV4QakfsppFT1z3ODpJ+2Ws/ujIT4PkPr62C3b6ima1GcCUucJqfZ2qG8G9WYaOE79Ziy93bJsfGFA6jxdqr337zf3V31qbqjAP7X0dsu5/VHgMwGp84RUe+ffH1797btX42anM5bebjm7P4p7HiB1B22qPfj3T29/+4+//GlYpZ2x9HbLBfsjWkDqDtpU+1Bu/2J3OziAGUuvwG8DSN3BwapcDnbLtircgNR57qZ2PmHy7ZbL9Ud0gNQXcrLBYbm6es3qKEDqC1nvgQr5OA6QepKQs+lWbfryixe7e2DIwq22MYICzwFSTzJmae+G8ovbm8a6/z28u97GCAo8B0g9SajL6FZt2vKLrg6jmc4Ny3XfrLYxguJOAVJPEu74YdUm3PH7uGpjhHMBpJ4kjPEh+ENx5e1+jNfGCGcCSD1JX5dBrtokW3b3R9iA1JOEuozpqs36GyNoiOcAqftYf2METepJQOomBTZGeNH+E2lZNr4wIHWTAgcV3H79Itpyqj8KPAlI3aTEQQW/WfiYtOLOAVI3Wb4xwqe33xU9mkSkAKmbFDiooDEC/Ufslqn+iDQg9Rx0UMEFAFLPobXy/2ZXX3w/PT/+xxI+XuM7D0g9lx+mBTbBwU99vPHFYLcc649m9A5A6oF40r3dEGEY40eLHxz8xMePgV+0waEC7wCkHogn3Zs/j7P6vcXvHPzUxx8cWeLf4FCBdwBSD8ST7ncHPn4/4f+m+9A3xz5+CPyyDQ4Vdx6QeiCedG/+HA4rCPd9H/jOwU99/DjGa4PDjQGpB+JJ9+bHZ1fvRjffB75z8FMfPwZeGxxuDEg9sDjpfoI2ONwYkHrATrrPPCoRdXbhkl/++tlf3WO8RngPIHUHpKW3W57vj+b0LkDqM0R2P2Atvd2yXHxhQOozzO1+4LD0dsvz/VHgXYDUZ5jb/cBh6e2WI/1R3D2A1OeY2f3AYentltn+iAQg9Tlmdj9wWHq7ZbY/IgFIfY5ylt5u2VaFG5D6DEeWvlvb6fx51h6HWS1H+qMx3gNIPYfxW3uTgwo0q3cBUp/QrdK3s/nnH7uXwZUP725wUIEC7wKkfkxw7u34fvdV+3I1uPLw9iYHFSjwLkDqEzrn3lmy7mV05eHdbQ4qUNw9gNSPCc69Ddr9r9oXDK48vK2DCs4XkPoxg3NHM5h3L4MrD2/roILzBaS+PnbLtircgNTzmNvD8LSieulBBRrgnYDUlzFTSp/Rsq1qSu8GpO5irL0/CnyJgwoUeC8gdRdj7f2klH75QQUKvBeQuoux9v64orrEQQWKuxOQuoux9v4w8DqooCogdRdj7f3RV70OKqgJSH197JZtVbgBqfs48nAFDyrQCO8GpO6DSczbLZuq5vR+QOpJ4hn6OfPu2+dOgfcDUk+RyNDP5uNd+9wp8H5A6kniGfr5fLxrnzvF3Q1IPUUiQz9n3rXP3daA1FMkMvSz+Xjtc7cxIPX1sVu2VeEGpB7hyLNNn41eZZ87DfF+QOoRooFfY587TeoXAFIPJM06vc+dx8cr8AsAqXckzLrjoXiXj1fgFwBSD8TNuuOheJ+PV9z9gNQ7Embd8VC8fPzWgNQ7EmadfyhePn5zQOrrY7dsq8INSD3KfKbdOCd+7hK7ZVPVGO8HpO7DOl9uDrtlS9WsfgEgdZP8c+dm8/Ge/eoV+AWA1E2Ic+dm8vGu/eoV+AWA1E3yz52by8f79qtX3P2A1E3yz52by8drv/qtAambEOfOzRTTa7/6rQGp5+A5d0771W8MSD2HsJv9MXMWv9ff7N+xW17SH2EAUs9iXK65XW2DQ83rFgJSz2Gcyq23waGc3FJA6jtmuSZ/g0N2AUeBXwpIfccs1+RvcMgu4CjwSwGp75jlmvwNDukFHMV9ISD1HbNck7/BoRZwtgakvmOWa7I3ONQCzuaA1E/xLNecogWcjQGpn2Is17z/qV+uSft47wKOxviFgNRzkI+/AEDqu5V8PPlAhQK/FJD6bh0fzz5QocAvBaS+W8fH0w9UKO4LAanv1vHxeqBia0Dqu1V8vB6o2ByQ+illfHyqZVsVbkDqp2SWXcxYeucDFRrilwJS95L/pWC3fKxqUr8YkHqKkFhvB/ZmCG8mfcNEL9/S2y3LxhcGpJ6iS6z3bv3m/qp/Ipqx9HbLx6oCvxiQeoqQWO/c+sOrv333atgLJ9/S2y1PVMV9KSD1JG1iPbj1T29/+4+//Im29HbL7v4IG5B6kjaxPmyc8GJ3O8z381Pzdsvu/ggbkHqSg8NCfdgt26pwA1JPcTe168Go/+I/F/boSNUIvxyQuo+yGyNoTl8AkLrJ8o0R7jAu/9kty8UXBqRusnxjhIO5gd3yoarAFwCkbrJ8YwSu2FJxXw5I3WT5xggHXsBumeqPSANSN1m8MUKb4UO85o7pj0gDUs9hWYbebtlWhRuQeg6xSvvpZ135eI3xywGp+5CPPztA6iYFNjhk6uoV+AKA1E0K+Himrl6BLwBI3aSEj2fq6hX35YDUTZb7eNXVbw1I3aSAj1dd/caA1HOQj78AQOo5THx8+5eg9/G/+NfpZ+XjKwFS9zGM8YfnDc5ht3yoalZfAJA6RThesLnjWx/fnkz4UyMM1t1fV6/AFwCkThGOF2xPm/rjx3A84Rcfmkl9/6a7rl6BLwBInaL9Zr+/DoHvfF0/0w/46+oV9+WA1Cmmd/z9rw4Cr7r6qoDUKcYxvvXx7cmEPx4EXnX1VQGpU+RM4jNbtlXhBqQ+R2fWp54t/DQ9Sr637t/PHCVvt3yoaogvAEg9wpxZnwa+/8CDN/Ca1JcApH5KyqyPy/Zj1j5cEV5Dvj4j8LLxZQGpnxI16/cv9om6fda+uyK89vl63fFbA1I/JWrW7w5S8/sc3jftFeG1z9drjN8akPopUbPe3NDN3d/I4b7vA998anjt8/VU4EUBQOqnRM16M4Q/u3o3Juj7wF+9bMf47rXP1yvwWwNSP8Vl1iPYLduqcANSP8UO/NwBg94e7VWN8EUAqa+P3fKoak5fBpB6kjCed9vd9VvavdjdN8N4//bic+cU+DKA1JOMJbZ3w5Z2tzfNVO/v4d3l584p8GUAqScJRfXddnftlnbd3nbNgD/U4C0/d05xLwJIPUm444ft7sIdv3/+XefOnQsg9SRhjA/BHzasvR3HeJ07dy6A1JP0RfXsdnc6d25jQOpJQlH9zHZ3s8/H0+fOiYWA1HMhk/DJlveqJndFAKnnslrgZefKAFIPJFdp2OqL/I0RFPgygNQD8VUaR/VF/sYICnwZQOqB+CqNo/qC2BhBcS8CSD0QX6VxVF9oY4StAakH4qs0fPWFNkbYHJB6wLlKk4Xdcrw/ggakHoiv0vx1P2VLVmLQGyNoiC8DSD2HceR2bYhitzyomtQXAqQ+oRvdu8coPnYvg0Ef3s09ZTC7EEOBLwRI/ZhQVxEeo2hfrgaDHt7OP2UwuxBDgS8ESH1CV1fRubPuZTTo4d38UwbzCzEU9zKA1I8JdRXhMYr2BYNBD2/nnzKoQoytAakfM5wsiGYw714Ggx7ezj9lUIUYWwNSXwsVYmwMSD2PqTuf93V0IYbG+DKA1H0wht5ueVA1qy8ESD1J3NnLx58LIPUUCWcvH38ugNSTxJ29fPy5AFJPkXD28vHnAkg9RcLZy8efCyD1tZCP3xiQupdCD1RohC8FSH197JaDqjl9MUDqSZwbI+TV1SvwxQCpJ/FtjJBZV6/AFwOknsS3MUJuXb3iXgqQehLfxgiqq98akHoS18YIqqvfHJB6ksUl93bL7v4IG5B6ErPk/uff/xj38Zl19RriiwFS91EoH69JfTlA6kl8+fg7jKeS2i3LxhcGpJ7CmY8/WKy3W+5UBb4cIPUkvnx8ZpJGcS8GSD2FMx9/YALslr39ETOA1FP48vGtFUB8rd7ZHzEDSH197JZtVbgBqZ/w/uPkB9u6jX598rZ8fCVA6lOGUwYPfoh79qSlt1vuVM3qywFSn3IbluJf7H+YeHba0tsty8cXBqQ+pT1srsvAtYePdSZ94tlpS2+33KkKfDlA6lOaeIece/tV35n0iWenLb3dclAV92KA1Kcc3vHBpE88O23p7ZZz+yMyAalPaU8ZDEP76+c/dSZ94tnpEnu75dz+iExA6utjt2yrwg1IPYe9Nzf3uUucRWi3HFSN8cUAqVOU3udOs/pygNQnxKvoS+9zp8CXA6Q+IV5FX3qfOwW+HCD1CfEq+uL73CnuxQCpT4hX0Wufu/MFpD4hXkWvfe7OF5D6hGIb1+v5+I0Bp0+z76njBQ9ycbZxp56P1xBfDlC6P/ueb+ntlltVk/qCgNL92fd8S2+3LBtfGFC6P/ueb+ntlltVgS8IKN2ffc+39HbLnaq4lwOU7s++51t6u+W5fgonoHR/9j3f0tstz/VTOAGpr4/dsq0KNyD1UxLp9cTbuS3n90dkAVJfyOT8+P2CUKrlTtXkrhwg9YVMAj+uAyVbblXZuYKA1JPESzMevnrdHyrcLe7cDk9KZhViKPAFAaknSWxw+EU7fb8fFnfGOz6rEEOBLwhIPUm8NOPhm24RZ1jcGQOfV4ihuJcDpJ4kXprR/ru544fFnf0Yr0KMjQGpJ0lscHj1sh/ju8WddkEovKFCjI0BqSdxlmaoEGNjQOonuEszwiw+6L/89bN377N2vcrtlkgAUp/iL82Ys/R2y62qWX1BQOpT/KUZc5beblk+vjAg9SkLNkaYsfR2y62qwBcEpD7FX5oxZ+ntljtVcS8HSH2KvzRjztLbLef2R2QCUp+yYGOEGUtvt5zbH5EJSH197JZtVbgBqedwWntx5OGGt+l8vIb4goDUfSTNe7JlaFJfFpB6Ep95zzlwUIEvCUg9hdO85xw4qMCXBKSexGfesw4cVNwLAlJP4TTvOnBwa0DqKXzmXQcObg5IfX3slm1VuAGp5/KDnY//fmLnjIMI7ZahIb4sIPVcpoEf6i6mgT/N3dstQ5P6soDUA/Hi+XYXhDdthXUT/odh14Pg4Cc+fp/QSR84qMAXBaQeiBfPN38O77aBH3Y9CA5+6uP3d3zywEEFvigg9UC8eP6ue7cP/PAMdHDwUx8/Bj6j2FJxLwlIPRAvnm/+HLx8uO/7wHcOfurjD/19tOV4fwQNSD0QL55vfnx29S58Zh/4zsFPfXwfeB04uDkg9UCxfQ2zW473R9CA1AOp4vkpc2nZ3fufsn28hviigNR9FMjHa1JfFpB6Emc+Pvl8vAJfFpB6Cmc+Pv18vAJfFpB6El8+PuP5eMW9KCD1FM58vJ6P3xqQegpnMb2ej98akPpa6Pn4jQGpe7Hz8aOPzzioQGN8UUDqC5GPPxdA6kmc+9wl6+oV+LKA1JP49rlL19Ur8GUBqSfx7XOXUVevuBcFpJ7Et8+d6uq3BqSexLXPnerqNweknmRxqt5u2d0fYQNST3KUqt+fMjiXqj/V7ZahIb4sIHWK/FMGky1rUl8YkPqEuGvPP2UwWVevwBcGpD4h7trzTxlM1tUr8IUBqU+Iu/b8UwbTSRrFvSwg9Qlx155/yqDq6rcGpD4h7tqzTxlUXf3mgNQnrFBgb7ec2R+RC0h9Altgf0Kmj9cIXxqQ+vpYLWtOXxyQepLko/MeH6/AFwekniSRj/f5eAW+OCD1JHFn7/XxintpQOpJ4s5ePv5cAKknSeTj5ePPBJB6EuXjLwOQehLS2Wfn4739ETYgdY7bnDWcrJZtVbgBqVMULMSwVeEGpD6hVCFG+oGKvP6IXEDqE4oVYmQcVCBKAlKfUK4QI+OgAlEQkPqEUoUYeqBia0DqE4oVYhw8UCE2YVng19zp8Ajo8m0uz/z0CoUYi/qjy5devrCx4kCXb3P5wsaKA12+zeULGysOdPk2ly9srDjQ5dtcvrAxcamgdgdEHVC7A6IOqN0BUQfU7oCoA2p3QNQBtTsg6oDaHTjiNmvd36TNJjz/uOQ3hNSi8/I7hO3dnJeHC52Xd5Vv7G+Ao6HVuH/+8f65M/8XCgIW/Ib2rAX35QtbDxc6L7/HZx/43wC6nRW5u/bVbY7X3/h/w8OX/3/t78Dd1305gu/yhy8/3nsv//S/bcFDuJb4DWDbWZO7m65qw0v7/877G9oiomv/5bddpaG///ftMOW9vAt8dy3xG8C3sx7L7vjuf777lm3LBa79lw8lh87Lu6pU7+WXf8cvGeM/fXuz8DfcLxzj7/xjfAiY9/I28Jc9xi+Zk992t+yS33C/cFa/4PLWkty4L+++3S96Vi+2A7U7IOqA2h0QdUDtDog6oHYHRB1QuwOiDqjdAVEH1O6AqANqd0DUAbU7IOqA2h0QdUDtDog6oHYHzov2MOQl718OqN2B80KBf6I0gf309n+A6/s2zfrz7/+7S3P+/Lota3v49z8AzQ/tHg83u4cv/6tLpTbvNR8Jn7gkULsD50Ub+G9f7B5e9hu1Pf943wT0tivReHh504m/e9f+6+FlJ7b1H82/mk/cvUj/+jMCtTtwXnR3/Lt2x652o6d2h77mx7aaqd3lr/ma77/qG6X/UxDavwyLykS3B7U7cF5MAv+7tqjlpg1uo4yBv+2+8PvAd9tBNd/0eVsBnQ+o3YHz4jTwJ3d8+zXQ3+r7O/6y7vYW1O7AeTH9qn/RDuDDGL8P9cNvhr8G7Rjf/Ct8onbvGVC7A+fF9I5/czirb7/xv/3swx3wL3+4Gb74x1n9ZX3TK/AxujH+kYLaHThnFHjx6EDtDog6oHYHRB1QuwOiDqjdAVEH1O6AqANqd0DUAbU7IOqA2h0QdUDtDog6/BODa2ep8loCxAAAAABJRU5ErkJggg==" alt="plot of chunk random-forest-varimp"/> </p>

<p>The roll sensor from the belt is by far the most important sensor regarding
to the random forest model, followed by the pitch sensor on the forearm and the 
yaw sensor on the belt.</p>

<p>There performance of the random forest model actually is so good that
training additional models (stacking,
ensemble learning) is probably superfluous, but just for the sake of practice,
I will also fit two more models.</p>

<h3>Multinomial regression model</h3>

<p>A multinomial regression model is much more rigorous and doesn&#39;t take into
account interactions between variables by default. Including all interactions
in the model formula resulted in the algorithm being unable to calculate
initial values for the parameters, so we will fit a model with only 
main effects here.</p>

<pre><code class="r">set.seed(21)
caret.mn &lt;- train(classe ~ roll_belt + pitch_belt + yaw_belt + total_accel_belt + 
    gyros_belt_x + gyros_belt_y + gyros_belt_z + accel_belt_x + accel_belt_y + 
    accel_belt_z + magnet_belt_x + magnet_belt_y + magnet_belt_z + roll_arm + 
    pitch_arm + yaw_arm + total_accel_arm + gyros_arm_x + gyros_arm_y + gyros_arm_z + 
    accel_arm_x + accel_arm_y + accel_arm_z + magnet_arm_x + magnet_arm_y + 
    magnet_arm_z + roll_dumbbell + pitch_dumbbell + yaw_dumbbell + gyros_dumbbell_x + 
    gyros_dumbbell_y + gyros_dumbbell_z + accel_dumbbell_x + accel_dumbbell_y + 
    accel_dumbbell_z + magnet_dumbbell_x + magnet_dumbbell_y + magnet_dumbbell_z + 
    roll_forearm + pitch_forearm + yaw_forearm + total_accel_forearm + gyros_forearm_x + 
    gyros_forearm_y + gyros_forearm_z + accel_forearm_x + accel_forearm_y + 
    accel_forearm_z + magnet_forearm_x + magnet_forearm_y + magnet_forearm_z, 
    data = dat.train, method = &quot;multinom&quot;, trace = FALSE)
</code></pre>

<p>Training is much faster (time: 5 minutes on my machine). After we have the model,
we can predict how the dumbbel curls were performed (classes)&hellip;</p>

<pre><code class="r">pred.caret.mn.train &lt;- predict(caret.mn$finalModel)
pred.caret.mn.val &lt;- predict(caret.mn$finalModel, newdata = dat.val)
</code></pre>

<p>&hellip; and inspect predcition accuracy in the training data:</p>

<pre><code class="r">confusionMatrix(pred.caret.mn.train, dat.train$classe)
</code></pre>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1452  204  111   97  105
##          B   63  562   67    9  100
##          C  165  227  685  184   93
##          D   79   32  144  645   91
##          E   64  214  110  114  787
## 
## Overall Statistics
##                                         
##                Accuracy : 0.645         
##                  95% CI : (0.633, 0.657)
##     No Information Rate : 0.285         
##     P-Value [Acc &gt; NIR] : &lt;2e-16        
##                                         
##                   Kappa : 0.55          
##  Mcnemar&#39;s Test P-Value : &lt;2e-16        
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity             0.796   0.4536    0.613    0.615    0.669
## Specificity             0.887   0.9537    0.873    0.935    0.904
## Pos Pred Value          0.737   0.7016    0.506    0.651    0.611
## Neg Pred Value          0.916   0.8792    0.914    0.925    0.924
## Prevalence              0.285   0.1935    0.174    0.164    0.184
## Detection Rate          0.227   0.0878    0.107    0.101    0.123
## Detection Prevalence    0.307   0.1251    0.211    0.155    0.201
## Balanced Accuracy       0.842   0.7037    0.743    0.775    0.787
</code></pre>

<p>The performance is actually much lower than compared with the random forest model.
We of course also have to look at the performance in the validation data set:</p>

<pre><code class="r">confusionMatrix(pred.caret.mn.val, dat.val$classe)
</code></pre>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1435  208  107  119  105
##          B   67  567   72    9  107
##          C  170  226  681  167  108
##          D   88   33  150  635  104
##          E   64  206  107  119  752
## 
## Overall Statistics
##                                         
##                Accuracy : 0.635         
##                  95% CI : (0.623, 0.647)
##     No Information Rate : 0.285         
##     P-Value [Acc &gt; NIR] : &lt;2e-16        
##                                         
##                   Kappa : 0.538         
##  Mcnemar&#39;s Test P-Value : &lt;2e-16        
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity             0.787   0.4573    0.610   0.6053    0.639
## Specificity             0.882   0.9506    0.873   0.9300    0.905
## Pos Pred Value          0.727   0.6898    0.504   0.6287    0.603
## Neg Pred Value          0.912   0.8795    0.914   0.9233    0.918
## Prevalence              0.285   0.1936    0.174   0.1638    0.184
## Detection Rate          0.224   0.0885    0.106   0.0991    0.117
## Detection Prevalence    0.308   0.1283    0.211   0.1577    0.195
## Balanced Accuracy       0.835   0.7039    0.741   0.7677    0.772
</code></pre>

<p>In the validation data set, performance is a little worse, but not by a lot.</p>

<p>In terms of variable importance, we see some differences compared to the 
random forest model:</p>

<pre><code class="r">plot(varImp(caret.mn))
</code></pre>

<p><img src="data:image/png;base64,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" alt="plot of chunk mn-varimp"/> </p>

<p>For the multinomial model (which assumes linear relationships and only 
main effects, in this case), the total acceleration as measured by the belt
sensor is the most important variable, by far. It is followed by the pitch
sensor and the roll sensor of the belt. The latter was identified as 
the most important variable by the random forest model.</p>

<h3>K nearest neighbor</h3>

<p>As a third (and final) algorithm, we will be training a k-nearest-neighbor
algorithm. Again, we will be using the standard settings from <code>caret</code>&#39;s 
<code>train()</code> function, which also figures out an acceptable parameter value
for the number of neighbors <em>k</em> that will be used for prediction later.</p>

<pre><code class="r">set.seed(55)
caret.knn &lt;- train(classe ~ roll_belt + pitch_belt + yaw_belt + total_accel_belt + 
    gyros_belt_x + gyros_belt_y + gyros_belt_z + accel_belt_x + accel_belt_y + 
    accel_belt_z + magnet_belt_x + magnet_belt_y + magnet_belt_z + roll_arm + 
    pitch_arm + yaw_arm + total_accel_arm + gyros_arm_x + gyros_arm_y + gyros_arm_z + 
    accel_arm_x + accel_arm_y + accel_arm_z + magnet_arm_x + magnet_arm_y + 
    magnet_arm_z + roll_dumbbell + pitch_dumbbell + yaw_dumbbell + gyros_dumbbell_x + 
    gyros_dumbbell_y + gyros_dumbbell_z + accel_dumbbell_x + accel_dumbbell_y + 
    accel_dumbbell_z + magnet_dumbbell_x + magnet_dumbbell_y + magnet_dumbbell_z + 
    roll_forearm + pitch_forearm + yaw_forearm + total_accel_forearm + gyros_forearm_x + 
    gyros_forearm_y + gyros_forearm_z + accel_forearm_x + accel_forearm_y + 
    accel_forearm_z + magnet_forearm_x + magnet_forearm_y + magnet_forearm_z, 
    data = dat.train, method = &quot;knn&quot;)
</code></pre>

<p>The final value for <em>k</em> chosen by the algorithm was <em>k=5</em>.
Training time was about 2 minutes.</p>

<p>In order to predict the class of the exercise (<code>classe</code> variable), we need some
additional code. The knn algorithm returns the probability for each class, so we
need to select the class that has the highest probability. In case two classes
have the same probability, we randomly choose a class (in order not to 
make a systematic error.)</p>

<pre><code class="r">pred.caret.knn.train.raw &lt;- predict(caret.knn$finalModel, newdata = dat.train[-1])
pred.caret.knn.train &lt;- apply(pred.caret.knn.train.raw, 1, function(i) colnames(pred.caret.knn.train.raw)[sample(which(i == 
    max(i)), size = 1)])
pred.caret.knn.val.raw &lt;- predict(caret.knn$finalModel, newdata = dat.val[-1])
pred.caret.knn.val &lt;- apply(pred.caret.knn.val.raw, 1, function(i) colnames(pred.caret.knn.val.raw)[sample(which(i == 
    max(i)), size = 1)])
</code></pre>

<p>Now we can inspect the performance in the training set:</p>

<pre><code class="r">confusionMatrix(pred.caret.knn.train, dat.train$classe)
</code></pre>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1762  614  371  270  258
##          B   29  578  386  249  248
##          C   16   28  339  274  226
##          D   11   12   16  252  212
##          E    5    7    5    4  232
## 
## Overall Statistics
##                                         
##                Accuracy : 0.494         
##                  95% CI : (0.482, 0.506)
##     No Information Rate : 0.285         
##     P-Value [Acc &gt; NIR] : &lt;2e-16        
##                                         
##                   Kappa : 0.339         
##  Mcnemar&#39;s Test P-Value : &lt;2e-16        
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity             0.967   0.4665   0.3035   0.2402   0.1973
## Specificity             0.670   0.8234   0.8971   0.9531   0.9960
## Pos Pred Value          0.538   0.3879   0.3839   0.5010   0.9170
## Neg Pred Value          0.981   0.8655   0.8591   0.8649   0.8465
## Prevalence              0.285   0.1935   0.1744   0.1638   0.1836
## Detection Rate          0.275   0.0903   0.0529   0.0394   0.0362
## Detection Prevalence    0.511   0.2327   0.1379   0.0785   0.0395
## Balanced Accuracy       0.818   0.6450   0.6003   0.5967   0.5966
</code></pre>

<p>Accuracy is even lower than for the multinomial model above already in the
training set, and things don&#39;t look any better in the validation set:</p>

<pre><code class="r">confusionMatrix(pred.caret.knn.val, dat.val$classe)
</code></pre>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1713  596  428  297  270
##          B   45  524  323  253  280
##          C   36   59  331  241  218
##          D   22   37   28  250  195
##          E    8   24    7    8  213
## 
## Overall Statistics
##                                         
##                Accuracy : 0.473         
##                  95% CI : (0.461, 0.485)
##     No Information Rate : 0.285         
##     P-Value [Acc &gt; NIR] : &lt;2e-16        
##                                         
##                   Kappa : 0.311         
##  Mcnemar&#39;s Test P-Value : &lt;2e-16        
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity             0.939   0.4226   0.2963    0.238   0.1811
## Specificity             0.653   0.8256   0.8953    0.947   0.9910
## Pos Pred Value          0.518   0.3677   0.3740    0.470   0.8192
## Neg Pred Value          0.964   0.8563   0.8576    0.864   0.8433
## Prevalence              0.285   0.1936   0.1744    0.164   0.1836
## Detection Rate          0.267   0.0818   0.0517    0.039   0.0333
## Detection Prevalence    0.516   0.2224   0.1382    0.083   0.0406
## Balanced Accuracy       0.796   0.6241   0.5958    0.593   0.5861
</code></pre>

<p>As a next step, we will do the stacking of the models by training the
meta-learner.</p>

<h2>Training the meta-learner (stacking the models)</h2>

<p>Although the performance of the initial random forest model above
was quite impressive, and the performance of the other models was 
significantly worse, we still use all the models for building an 
ensemble model by stacking the predictions together. 
For that purpose, we will create a new data frame containing the 
predictions of the individual models <em>on the training set</em>, as well
as the correct classes <em>of the training set</em>. Note that this is not the way it was
shown in the lectures, but on other sources I consulted. It allows to train
the stacked model on the training data alone, and evaluate the
performance of the stacked model on the validation data set.</p>

<pre><code class="r">dat.stack.train &lt;- data.frame(classe = dat.train$classe, pred.caret.rf = pred.caret.rf.train, 
    pred.caret.mn = pred.caret.mn.train, pred.caret.knn = pred.caret.knn.train)  #,&#39;pred.fit.svm&#39;=pred.fit.svm.train)
</code></pre>

<p>Additionally, for getting a (kind of) out-of-sample error rate,
we build a data frame of the predictions of the indivdual models
on the validation set, together with the true classes of the 
validation data set.</p>

<pre><code class="r">dat.stack.val &lt;- data.frame(classe = dat.val$classe, pred.caret.rf = pred.caret.rf.val, 
    pred.caret.mn = pred.caret.mn.val, pred.caret.knn = pred.caret.knn.val)  #,&#39;pred.fit.svm&#39;=pred.fit.svm.val1)
</code></pre>

<p>In order to use this for predicting the classes with the meta leaner,
we actually need to generate dummy variables for all of the predictors,
which is achieved by the <code>model.matrix()</code> function:</p>

<pre><code class="r">## convert to model matrix (for prediction of new data):
dat.stack.val.mm &lt;- model.matrix(classe ~ ., data = dat.stack.val)
head(dat.stack.val.mm)
</code></pre>

<pre><code>##    (Intercept) pred.caret.rfB pred.caret.rfC pred.caret.rfD pred.caret.rfE
## 6            1              0              0              0              0
## 9            1              0              0              0              0
## 11           1              0              0              0              0
## 15           1              0              0              0              0
## 26           1              0              0              0              0
## 33           1              0              0              0              0
##    pred.caret.mnB pred.caret.mnC pred.caret.mnD pred.caret.mnE
## 6               0              0              0              0
## 9               0              0              0              0
## 11              0              0              0              0
## 15              0              0              0              0
## 26              0              0              0              0
## 33              0              0              0              0
##    pred.caret.knnB pred.caret.knnC pred.caret.knnD pred.caret.knnE
## 6                0               0               0               0
## 9                0               0               0               0
## 11               0               0               0               0
## 15               0               0               0               0
## 26               0               0               0               0
## 33               0               0               0               0
</code></pre>

<p>So now we have the data to train (and also test) the meta learner,
so we can start with training it on the data frame that contains 
only the training data. We will use a random forest model as the meta
learner in this case.</p>

<pre><code class="r">set.seed(78)
fit.stack.rf &lt;- train(classe ~ ., data = dat.stack.train, method = &quot;rf&quot;)
</code></pre>

<p>Now we create a vector of predictions of the ensemble model, both for the
training data (which was used to train the meta learner), as well as
for the validation data:</p>

<pre><code class="r">pred.fit.stack.rf.train &lt;- predict(fit.stack.rf$finalModel)
pred.fit.stack.rf.val &lt;- predict(fit.stack.rf$finalModel, newdata = dat.stack.val.mm)
</code></pre>

<p>Now let&#39;s look at the in-sample performance of the meta learner in the
training data:</p>

<pre><code class="r">confusionMatrix(pred.fit.stack.rf.train, dat.stack.train$classe)
</code></pre>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1816   24    0    0    0
##          B    4 1201   21    1    4
##          C    2   12 1090   21    3
##          D    0    0    6 1026    6
##          E    1    2    0    1 1163
## 
## Overall Statistics
##                                        
##                Accuracy : 0.983        
##                  95% CI : (0.98, 0.986)
##     No Information Rate : 0.285        
##     P-Value [Acc &gt; NIR] : &lt;2e-16       
##                                        
##                   Kappa : 0.979        
##  Mcnemar&#39;s Test P-Value : NA           
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity             0.996    0.969    0.976    0.978    0.989
## Specificity             0.995    0.994    0.993    0.998    0.999
## Pos Pred Value          0.987    0.976    0.966    0.988    0.997
## Neg Pred Value          0.998    0.993    0.995    0.996    0.998
## Prevalence              0.285    0.193    0.174    0.164    0.184
## Detection Rate          0.284    0.188    0.170    0.160    0.182
## Detection Prevalence    0.287    0.192    0.176    0.162    0.182
## Balanced Accuracy       0.995    0.982    0.984    0.988    0.994
</code></pre>

<p>The performance is quite impressive, and it is higher than each of the 
individual models (even the random forest model). But in order to really
judge the performance, we need to look at the predictions of the meta
learner for the validation data:</p>

<pre><code class="r">confusionMatrix(pred.fit.stack.rf.val, dat.stack.val$classe)
</code></pre>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1809   16    0    0    0
##          B    3 1206   16    1    2
##          C   10   18 1100   23    5
##          D    1    0    1 1024    5
##          E    1    0    0    1 1164
## 
## Overall Statistics
##                                         
##                Accuracy : 0.984         
##                  95% CI : (0.981, 0.987)
##     No Information Rate : 0.285         
##     P-Value [Acc &gt; NIR] : &lt; 2e-16       
##                                         
##                   Kappa : 0.98          
##  Mcnemar&#39;s Test P-Value : 1.22e-07      
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity             0.992    0.973    0.985    0.976    0.990
## Specificity             0.997    0.996    0.989    0.999    1.000
## Pos Pred Value          0.991    0.982    0.952    0.993    0.998
## Neg Pred Value          0.997    0.993    0.997    0.995    0.998
## Prevalence              0.285    0.194    0.174    0.164    0.184
## Detection Rate          0.282    0.188    0.172    0.160    0.182
## Detection Prevalence    0.285    0.192    0.180    0.161    0.182
## Balanced Accuracy       0.994    0.984    0.987    0.987    0.995
</code></pre>

<p>The performance measures show a really good performance of the stacked
model even in the validation data. Accuracy is as high as .984, which 
again is higher as for each of the individual models.</p>

<h2>Testing the meta learner: Out-of-sample error</h2>

<p>As a final estimate for the out-of-sample error, we now will look at the
performance of the meta learner in the testing data, which we have not 
touched yet.</p>

<p>First, we will create vectors of predictions for each of the individual
models, just like above:</p>

<pre><code class="r">pred.caret.rf.test &lt;- predict(caret.rf$finalModel, newdata = dat.test)

pred.caret.mn.test &lt;- predict(caret.mn$finalModel, newdata = dat.test)

pred.caret.knn.test.raw &lt;- predict(caret.knn$finalModel, newdata = dat.test[-1])
pred.caret.knn.test &lt;- apply(pred.caret.knn.test.raw, 1, function(i) colnames(pred.caret.knn.test.raw)[sample(which(i == 
    max(i)), size = 1)])
</code></pre>

<p>Then we will just build a data frame containing these predictions of the 
individual model in the testing data, together with the <code>classe</code> variable
of the testing data. We again need to build dummy variables for the meta
learner to be able to use the data.</p>

<pre><code class="r">dat.stack.test &lt;- data.frame(classe = dat.test$classe, pred.caret.rf = pred.caret.rf.test, 
    pred.caret.mn = pred.caret.mn.test, pred.caret.knn = pred.caret.knn.test)

## convert to model matrix (for prediction of new data):
dat.stack.test.mm &lt;- model.matrix(classe ~ ., data = dat.stack.test)
</code></pre>

<p>Now we can make the predictions for the testing data:</p>

<pre><code class="r">pred.fit.stack.rf.test &lt;- predict(fit.stack.rf$finalModel, newdata = dat.stack.test.mm)
</code></pre>

<p>To finally get an estimate of the out-of-sample error, with data that we
didn&#39;t touch yet, we compute the performance measures on the testing data:</p>

<pre><code class="r">confusionMatrix(pred.fit.stack.rf.test, dat.stack.test$classe)
</code></pre>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1813   21    0    0    0
##          B    2 1209    9    1    1
##          C    8    9 1109   20    1
##          D    0    0    0 1027    6
##          E    1    0    0    1 1168
## 
## Overall Statistics
##                                        
##                Accuracy : 0.988        
##                  95% CI : (0.984, 0.99)
##     No Information Rate : 0.285        
##     P-Value [Acc &gt; NIR] : &lt;2e-16       
##                                        
##                   Kappa : 0.984        
##  Mcnemar&#39;s Test P-Value : NA           
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity             0.994    0.976    0.992    0.979    0.993
## Specificity             0.995    0.997    0.993    0.999    1.000
## Pos Pred Value          0.989    0.989    0.967    0.994    0.998
## Neg Pred Value          0.998    0.994    0.998    0.996    0.998
## Prevalence              0.285    0.193    0.175    0.164    0.184
## Detection Rate          0.283    0.189    0.173    0.160    0.182
## Detection Prevalence    0.286    0.191    0.179    0.161    0.183
## Balanced Accuracy       0.995    0.987    0.992    0.989    0.996
</code></pre>

<p>Even in the testing data, the stacked model achieves an accuracy of .988,
which is even higher than the accuracy both in the training and 
in the validation data set (almost certainly a random fluctuation).</p>

</body>

</html>

